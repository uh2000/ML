{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uh2000/ML/blob/main/Scaling_Unbalanced_Data_Introduction_Intro_and_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfOTu0aKSw7S"
      },
      "source": [
        "# Theory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP\n",
        "# install python at version 3.10\n",
        "!apt-get install python3.10\n",
        "\n",
        "# update symbolic links to the newly installed python version\n",
        "!ln -sf /usr/bin/python3.10 /usr/bin/python\n",
        "!ln -sf /usr/bin/python3.10 /usr/bin/python3\n",
        "\n",
        "# install numpy 1.23.5\n",
        "%pip install numpy==1.23.5\n",
        "# install scikit 1.2.2\n",
        "%pip install scikit-learn==1.2.2"
      ],
      "metadata": {
        "id": "VDzFrkBZ0jNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVCD1K5XS0Hc"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "One of the tasks of a data scientist is to guide the training process of  Machine Learning algorithms. <br>\n",
        "This operation does require the definition of a training strategy, which might involve some preprocessing operations. <br>\n",
        "Preprocessing operations are meant to ease the training phase of our models, and sometimes they are essentials! <br>\n",
        "In today's exercises, we will see a practical example. <br>\n",
        "\n",
        "Now we show you some Sklearn preprocessing functions <a href = \"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\"> [link] </a>.\n",
        "\n",
        "Usually, scaling operations are performed column-wise, i.e., each column is standardized independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1_mFKGNSzeB"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#define our array\n",
        "data = np.array([[0, 0], [0, 0], [1, 2], [1, 1]])\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSfdAKMUeaI"
      },
      "source": [
        "A first example of preprocessing is the MinMaxScaler <a href = https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler> [link] </a>.\n",
        "\n",
        "The MinMaxScaler transforms the features in a fixed range scale, for example $[0, 1]$ (default) <br>\n",
        "\n",
        "The MinMaxScaler is defined as follows:\n",
        "\n",
        "    min = 0\n",
        "    max = 1\n",
        "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "    X_scaled = X_std * (max - min) + min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_UpQipUVl7W"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#define the scaler\n",
        "scaler1 = MinMaxScaler()\n",
        "\n",
        "#fit the scaler\n",
        "scaler1.fit(data)\n",
        "\n",
        "print(scaler1.transform(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBSbOFq7WLdB"
      },
      "source": [
        "Similarly, we can use the the StandardScaler <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\"> [link] </a>. <br>\n",
        "\n",
        "This scaler standardizes features by removing the mean and scaling to unit variance. For example, given an array $X$, the function outputs the following\n",
        "$X_{std} = \\frac{(X - \\mu)}{\\sigma}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF-l7L0fWKoa"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#define the scaler\n",
        "scaler2 = StandardScaler()\n",
        "\n",
        "#fit the scaler\n",
        "scaler2.fit(data)\n",
        "\n",
        "print(scaler2.transform(data))\n",
        "\n",
        "#we can plot also the mean of each axis\n",
        "print(scaler2.mean_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YycHdgGuYAHb"
      },
      "source": [
        "If you apply preprocessing techniques, you need to use them on every sets (i.e., training, validation, and testing) **consistently**. <br>\n",
        "When using preprocessing techniques, you need to apply the *fit* operation to the **training set only**. <br>\n",
        "The validation set can be used to find the best hyperparameter setting of the preprocessing function (if any). <br>\n",
        "The test set, instead, cannot be used for any reason. <br>\n",
        "We can now see a simple example of what might go wrong if do not respect such a requirement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQy452ymY4s3"
      },
      "source": [
        "#we first define a training array\n",
        "X_tr = np.array([[0, 1, 2, 3, 4, 5, 6]]).T\n",
        "\n",
        "#the ground truth is defined as a binary classification task, where the sample\n",
        "# is 0 if <=3, 1 otherwise\n",
        "y_tr = np.array([0, 0, 0, 0, 1, 1, 1])\n",
        "\n",
        "#we define the test set with the same rules\n",
        "X_te = np.array([[0, 0, 3, 20]]).T\n",
        "y_te = np.array([0, 0, 0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmjcUHPgaUyi"
      },
      "source": [
        "X_tr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nRfOjwnZk4k"
      },
      "source": [
        "#we might want to scale all of the training data in the range [0, 1]\n",
        "scaler_tr = MinMaxScaler()\n",
        "scaler_tr.fit(X_tr)\n",
        "X_tr_scaled = scaler_tr.transform(X_tr)\n",
        "print(X_tr_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltUMs3hHacYk"
      },
      "source": [
        "A classifier might define a rule that, given a scaled point $x_i$, returns 0 if it is lower or equale than 0.5, 1 otherwise.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjveTT-DatWa"
      },
      "source": [
        "#we now wrongly fit a new scaler over the test set.\n",
        "#let's see what happens\n",
        "print(X_te)\n",
        "scaler_te = MinMaxScaler()\n",
        "scaler_te.fit(X_te)\n",
        "X_te_scaled = scaler_te.transform(X_te)\n",
        "print(X_te_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PbcLpyUbDXt"
      },
      "source": [
        "The classifier we previously defined might work well in most cases, but it misclassify $x_i = 3$: now its scaled value is 0.15 and not 0.5. <br>\n",
        "If we use the correct process, i.e., transforming the testing data with the scaler fitted on the training data, the output is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJqynHDZbztb"
      },
      "source": [
        "#we now wrongly fit a new scaler over the test set.\n",
        "#let's see what happens\n",
        "print(X_te)\n",
        "X_te_scaled = scaler_tr.transform(X_te)\n",
        "print(X_te_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHK-ScxAb7SJ"
      },
      "source": [
        "### Undersampling / Oversampling\n",
        "\n",
        "Sometimes we might face unbalanced datasets. <br>\n",
        "In classification tasks, this means that the number of samples per class are not equal among the classes. <br>\n",
        "These dataset can produce *deceiving* results, and today we are going to experiment with an ad hoc example. <br>\n",
        "\n",
        "Let $X, y$ be a dataset defined over two classes $(c_0, c_1)$.\n",
        "Let $c_0$ be the minority class, and $c_1$ the majority one. <br>\n",
        "We might face different unbalance levels, for example weak unbalance (e.g., $c_0 = 100$ and $c_1 = 110$), or heavily unbalance (e.g., $c_0 = 100$ and $c_1=1000$). <br>\n",
        "In general, when we have unbalanced data, we can apply two mitigation strategies:\n",
        "\n",
        "\n",
        "1.   *Oversampling*: we use a generator $G$ to produce new samples belonging to $c_0$. In this way, we make $c_0$ bigger and closer to $c_1$.\n",
        "2.   *Undersampling*: we use a generator $G$ to select a subset of samples belonging to $c_1$. In this way, we make $c_1$ smaller and closer to $c_0$.\n",
        "\n",
        "One simple way to implement such strategies in sci-kit learn is to use the **class_weight** parameter of many learning algorithms ([Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression) and [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html?highlight=perceptron#) are among these). **class_weight** multiplies the error, during learning, on every example of that class. We can use that to weight the errors on the minority class such that overall they count as much as the ones on the majority class (see the explanation of the 'balanced' value of the parameter **class_weight**). <br>\n",
        "We now turn our attention to techniques that directly change the data at preprocessing time.\n",
        "We can find several techniques to define $G$. Today we show an example of oversampling and undersampling $G$. Remember: when defining $G$ we can explicitly define the magnitude of the sampling strategy. <br>\n",
        "For example, in the case of oversampling:\n",
        "\n",
        "\n",
        "*   100% balancy: from $c_0 = 100$ and $c_1=1000$ to $c_0 = 1000$ and $c_1=1000$.\n",
        "*   50% balancy: from $c_0 = 100$ and $c_1=1000$ to $c_0 = 500$ and $c_1=1000$.\n",
        "\n",
        "\n",
        "In our exercises we use generators $G$ developed by *imbalance learn* library, a library written on top of *sklearn* <a href = \"https://imbalanced-learn.org/stable/index.html\"> [link] </a>. <br>\n",
        "We are going to use [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html?highlight=smote#imblearn.over_sampling.SMOTE) (an oversampler), and [RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html?highlight=randomundersampler) (an undersampler).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-QyknDGfJhZ"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#we generate a two-classes dataset. 1000 samples.\n",
        "# c0 = 100 samples, c1 = 900 samples\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative= 1, n_redundant= 0,\n",
        "                           flip_y=0, n_features=1, n_clusters_per_class=1, n_samples=1000, random_state=123)\n",
        "\n",
        "#print the first 10 samples\n",
        "print(X[:10], '\\n' ,y[:10])\n",
        "\n",
        "#we can see the distribution of the two classes\n",
        "print(len(X))\n",
        "print(f\"Class 0:{np.sum(y == 0)}\")\n",
        "print(f\"Class 1:{np.sum(y == 1)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRnZ20-vgIH5"
      },
      "source": [
        "#we can try to use the undersampling strategy\n",
        "#by using 0.5 as ratio, we ask the classifier to make the minority class 50% of the size of the majority one\n",
        "#here the minority class size is fixed. We change the majority\n",
        "undersampler = RandomUnderSampler(random_state = 123,\n",
        "                                  sampling_strategy= 0.5)\n",
        "\n",
        "#resample\n",
        "X_under, y_under = undersampler.fit_resample(X, y)\n",
        "\n",
        "#we can see the distribution of the two classes\n",
        "print(len(X_under))\n",
        "print(f\"Class 0:{np.sum(y_under == 0)}\")\n",
        "print(f\"Class 1:{np.sum(y_under == 1)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZuc93tLhLHr"
      },
      "source": [
        "#we can try with the oversampling strategy\n",
        "#by using 0.5 as ratio, we ask the classifier to make the minority class 50% of the size of the majority one\n",
        "# here the majority class size is fixed. We vary the minority\n",
        "oversampler = SMOTE(random_state = 123, sampling_strategy= 0.5)\n",
        "\n",
        "#resample\n",
        "X_over, y_over = oversampler.fit_resample(X, y)\n",
        "\n",
        "#we can see the distribution of the two classes\n",
        "print(len(X_over))\n",
        "print(f\"Class 0:{np.sum(y_over == 0)}\")\n",
        "print(f\"Class 1:{np.sum(y_over == 1)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-AyO22oYvgp"
      },
      "source": [
        "--------------\n",
        "# Exercises\n",
        "\n",
        "### Exercise 1: Scaling\n",
        "\n",
        "We start by loading the dataset that we use in our exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx2CiV8uYyWs"
      },
      "source": [
        "from sklearn.datasets import load_wine #load our target dataset\n",
        "import numpy as np\n",
        "\n",
        "#load the dataset\n",
        "dataset = load_wine()\n",
        "\n",
        "#extract X and y\n",
        "X = dataset.data\n",
        "y = dataset.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN-AKBVnCOUk"
      },
      "source": [
        "The dataset contains three classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhBRGTIwCJLT"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23nXacv6ahU9"
      },
      "source": [
        "We now have a look on how the data looks like. <br>\n",
        "Print the X and y shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROa-fqLUaw3P"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.1 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KThOCq9fa93A"
      },
      "source": [
        "The dataset contains $178$ samples and $13$ features. <br>\n",
        "The first thing to do, is to create the train, val, and test partitions. <br>\n",
        "\n",
        "To do so, start by defining a split *X_train_val* and *X_test*, with $80\\%$ of the samples in the *X_train_val* set. <br>\n",
        "Then, get *X_train* and *X_val*, where *X_train* contains the $90\\%$ of the samples. <br>\n",
        "\n",
        "Set the **random_state** to $123$ in this phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-We288DbvrB"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.2 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y75YhHVPdci3"
      },
      "source": [
        "Now define a LogisticRegression with default parameters (and **random state** = $123$), fit it on the training data, and evaluate both on the training and validation sets. <br>\n",
        "Use the *accuracy score* as the evaluation metric. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6EneDDmfCM2"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.3 here\n",
        "#\n",
        "# expected output: 0.968503937007874 0.8666666666666667\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PID-oueTCs2b"
      },
      "source": [
        "The previous cell returns the following message:\n",
        "\n",
        "    ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "\n",
        "    Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "    Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "    extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
        "    (0.9824561403508771, 0.8666666666666667)\n",
        "\n",
        "Sklearn suggest you to increase the max number of iterations, or to scale the data. <br>\n",
        "We go for the second way. <br>\n",
        "Using $X_{tr}$, fit a *standard scaler*, and transform $X_{tr}$, $X_{val}$, and $X_{te}$ using the fitted scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iygiO-IKDr4F"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.4 here\n",
        "#\n",
        "# The new variables are called X_train_scaled, X_val_scaled, and X_test_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70FrA5jEnAo"
      },
      "source": [
        "Print $\\mu$ of the scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5_Q5TAEwGD"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.5 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwDygBbkFGwy"
      },
      "source": [
        "We now check if the preprocessing solved our issue encountered in Ex 2.1.3. <br>\n",
        "Train the logistic regression with default parameters (and random state =  123) on the scaled data and print the accuracy on the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6T1Hy9rFZz-"
      },
      "source": [
        "#\n",
        "#   complete EX 2.1.6 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5V5G2OKQ4g"
      },
      "source": [
        "# Exercise 2.2: Dealing with unbalanced data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FrRq1PdKhHg"
      },
      "source": [
        "We first define a new ad hoc dataset. This time we fix the random_state to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHoPHU5YKl0E"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_toy, y_toy = datasets.make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDjx4Ay3LQQ7"
      },
      "source": [
        "Print the number of samples, and the number of features. <br>\n",
        "\n",
        "Split the data: first create a test set ($20\\%$), and then split the remaining data into training and validation sets ($90\\%$ training size, $10\\%$ validation set size). Again, set the **random_state** to $123$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFgKKWpDKUD9"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.1 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJy_dpETMbDq"
      },
      "source": [
        "Train a Logistic Regression with default parameters (**random state** = $123$) on the training set, and evaluate it on the validation set. <br>\n",
        "Use the *accuracy score* as evaluation metric.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXyTEpDCMaYH"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.2 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KWddVioNK6o"
      },
      "source": [
        "We just obtained a wonderful score! Almost perfect prediction. <br>\n",
        "However, in datascience, we always need to verify our scores. <br>\n",
        "We can start by counting the number of samples of each class contained in the training and validation sets (separately)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUnq7PcNvHn"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.3 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_qjWWaKN-82"
      },
      "source": [
        "There is a high unbalance! For example, in the validation we have 791 samples belonging to class 0, and only 9 to class 1! Can this affect our scores? <br>\n",
        "To understand how good is an accuracy of $0.99$, do the following:\n",
        "create an array $y_{dummy} = 0$, where $|y_{dummy}| = |y_{val}|$. <br>\n",
        "$|x|=n$ is the cardinality of x, in this example the vector x has n elements.\n",
        "<br>\n",
        "Once you defined $y_{dummy}$, compute the validation accuracy between the true validation labels (i.e., $y_{val}$) and $y_{dummy}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHvteD-VO5Ni"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.4 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtZEG434Po-2"
      },
      "source": [
        "Ok, that is not exactly amazing. <br>\n",
        "The accuracy score is maybe not the best metric for unbalanced data. <br>\n",
        "Let's use the [$F_1$ score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) instead. <br>\n",
        "Print both validation F$_1$ of the $lr$ predictions and the $y_{dummy}$ with respect to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDzF3YmUQXeh"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.5 here\n",
        "#\n",
        "# Expected output: 0.61 for the logistic regression, and 0.0 for the dummy array."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnEVe5cuQ8xs"
      },
      "source": [
        "F$_1$ gives a more useful picture of our classifiers. <br>\n",
        "Now we just need to improve the classification performance. <br>\n",
        "We start by plotting the data. How does training data look like?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJPDvERhRJy3"
      },
      "source": [
        "# scatter plot of examples by class label\n",
        "from collections import Counter\n",
        "from numpy import where\n",
        "\n",
        "counter = Counter(y_train)\n",
        "\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_train == label)[0]\n",
        "\tplt.scatter(X_train[row_ix, 0], X_train[row_ix, 1], label=str(label))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_80GeGPRtUp"
      },
      "source": [
        "There is a big blue cluster, and a few orange points. <br>\n",
        "Let's use SMOTE to oversample the minority class. <br>\n",
        "First, let's apply SMOTE with **sampling_strategy**=$0.05$ and **random state** = $123$ over the training set, and obtain new variables *X_train_over*, *y_train_over*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OvmUDeASQq5"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.6 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34FAN7m2NuSC"
      },
      "source": [
        "After the oversampling, print the number of samples contained in every class of the training. Is the balancy situation improved compared to the raw training set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F3W0UquOD7_"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.7 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3gmT75STmiM"
      },
      "source": [
        "We can see how the training data changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oMhZERcTgAZ"
      },
      "source": [
        "counter = Counter(y_train_over)\n",
        "\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_train_over == label)[0]\n",
        "\tplt.scatter(X_train_over[row_ix, 0], X_train_over[row_ix, 1], label=str(label))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i752RsgTr2Z"
      },
      "source": [
        "SMOTE creates novel artificial examples. Here is how it works (from the [paper](https://arxiv.org/pdf/1106.1813.pdf) where it was first described) in a nutshell:  \n",
        "```\n",
        "The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the\n",
        "line segments joining any/all of the k minority class nearest neighbors.\n",
        "Define a new Logistic Regression (random state = 123), train it over the oversampled\n",
        "```\n",
        "\n",
        "Define a new Logistic Regression with **random_state** = $123$, train it over the oversampled training data, and calculate the new validation F$_1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C2gAQmqUCFe"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.8 here\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvuihDKnUKv_"
      },
      "source": [
        "Definitely better! Good job. <br>\n",
        "We can now try another way to solve the unbalance problem: instead of increasing the number of samples belonging to the minority class (i.e., class 0), we reduce the number of sample of the majority one (i.e., class 1). <br>\n",
        "Remember: the fit operations can be done only in the training data. <br>\n",
        "\n",
        "Use the *RandomUnderSampler* with default parameters to undersample the training data. <br>\n",
        "Call the new variables and obtain new variables X_train_under, y_train_under.\n",
        "Print the distribution of the two undersampled classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8GJKEsiQrIi"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.9 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iaXq0-eQYuR"
      },
      "source": [
        "Train a LogisticRegression using the undersampled training data, and evaluate with the F$_1$ on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcz1rWg2QwtE"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.10 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljLnzFYZQ50w"
      },
      "source": [
        "We do not obtain a great performance, with only 73 samples per class. <br>\n",
        "Let's try another sampling variation. <br>\n",
        "In our first sampling solution we add samples to the minority class until the size is the same of the majority one, and in the second case we did the opposite, by reducing the majority class. <br>\n",
        "Sampling solutions allow us to control the amount of *unbalancement* we want. <br>\n",
        "\n",
        "For example, we can create a new undersampling strategy with **sampling_strategy = 0.5**. This ratio indicates that the minority class size will be $1/2$ of the majority class size. <br>\n",
        "Print the distribution of the new training data, then train a new LogisticRgression and evaluate it (with F$_1$) on the validation data. <br>\n",
        "Play with the value of **sampling_strategy**, what is the best F$_1$ that you can get on the validation set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFSvqC4MSREq"
      },
      "source": [
        "#\n",
        "#   complete EX 2.2.11 here\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}